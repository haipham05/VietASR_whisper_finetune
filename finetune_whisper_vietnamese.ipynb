{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6c2700",
   "metadata": {},
   "source": [
    "# Fine-tuning Whisper for Vietnamese ASR\n",
    "This notebook demonstrates how to fine-tune OpenAI's Whisper model on a Vietnamese speech dataset using Hugging Face Transformers. The workflow includes environment setup, data loading, preprocessing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad522c7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Install the required libraries: `transformers`, `datasets`, `torchaudio`, and `jiwer` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3847e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torchaudio jiwer --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c0ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haipham2407/miniconda3/envs/nlp_prj/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, Audio, load_from_disk\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7116a182",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "Assume your CSV files (`fpt_train.csv`, `fpt_val.csv`, `fpt_test.csv`) are in the current directory and contain columns: `path` (audio file path) and `transcription` (text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc626b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = ds.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "    return ds\n",
    "\n",
    "train_dataset = load_csv_to_dataset(\"fpt_train.csv\")\n",
    "val_dataset = load_csv_to_dataset(\"fpt_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6418871",
   "metadata": {},
   "source": [
    "## 3. Load Whisper Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa97188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"vi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786aa8e3",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Function\n",
    "Tokenize transcriptions and prepare input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9769f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    if isinstance(audio, list):\n",
    "        input_features = [processor.feature_extractor(a[\"array\"], sampling_rate=a[\"sampling_rate\"]).input_features[0] for a in audio]\n",
    "    else:\n",
    "        input_features = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    if isinstance(batch[\"transcription\"], list):\n",
    "        labels = [processor.tokenizer(text).input_ids for text in batch[\"transcription\"]]\n",
    "    else:\n",
    "        labels = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return {\"input_features\": input_features, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data: 100%|██████████| 20735/20735 [02:03<00:00, 167.95 examples/s]\n",
      "\n",
      "Processing validation data: 100%|██████████| 2592/2592 [00:13<00:00, 190.81 examples/s]\n",
      "\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 2592/2592 [00:03<00:00, 845.67 examples/s]\n",
      "Saving the dataset (0/40 shards):   0%|          | 0/20735 [00:00<?, ? examples/s]\n",
      "Saving the dataset (40/40 shards): 100%|██████████| 20735/20735 [01:23<00:00, 247.90 examples/s]\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2592/2592 [00:03<00:00, 861.85 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    if isinstance(audio, list):\n",
    "        input_features = [processor.feature_extractor(a[\"array\"], sampling_rate=a[\"sampling_rate\"]).input_features[0] for a in audio]\n",
    "    else:\n",
    "        input_features = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    if isinstance(batch[\"transcription\"], list):\n",
    "        labels = [processor.tokenizer(text).input_ids for text in batch[\"transcription\"]]\n",
    "    else:\n",
    "        labels = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    num_proc=1,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=False,\n",
    "    desc=\"Processing training data\"\n",
    ")\n",
    "gc.collect()\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    num_proc=1,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=False,\n",
    "    desc=\"Processing validation data\"\n",
    ")\n",
    "gc.collect()\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "val_dataset_processed = val_dataset.save_to_disk(\"val_dataset_processed\")\n",
    "train_dataset_processed = train_dataset.save_to_disk(\"train_dataset_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f677196",
   "metadata": {},
   "source": [
    "## 5. Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa06657",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cfa136",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(\"train_dataset_processed\")\n",
    "val_dataset = load_from_disk(\"val_dataset_processed\")\n",
    "test_dataset = load_from_disk(\"test_dataset_processed\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-vi-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=True,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    try:\n",
    "        from jiwer import wer\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        wer_score = wer(label_str, pred_str)\n",
    "        del pred_str, label_str\n",
    "        gc.collect()\n",
    "        return {\"wer\": wer_score}\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        return {\"wer\": 1.0}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39476a",
   "metadata": {},
   "source": [
    "## 6. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cec5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7776' max='7776' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7776/7776 4:13:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.864725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.668017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.586669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>0.516293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.445573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.451866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.412356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haipham2407/miniconda3/envs/nlp_prj/lib/python3.11/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7776, training_loss=0.43403253560203586, metrics={'train_runtime': 15227.2317, 'train_samples_per_second': 4.085, 'train_steps_per_second': 0.511, 'total_flos': 1.79514548269056e+19, 'train_loss': 0.43403253560203586, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bc90e",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cbb01d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 2592/2592 [00:15<00:00, 170.58 examples/s]\n",
      "Processing validation data: 100%|██████████| 2592/2592 [00:15<00:00, 170.58 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 2592/2592 [00:03<00:00, 746.66 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_csv_to_dataset(\"fpt_test.csv\")\n",
    "test_dataset = test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    num_proc=1,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=False,\n",
    "    desc=\"Processing validation data\"\n",
    ")\n",
    "gc.collect()\n",
    "test_dataset_processed = test_dataset.save_to_disk(\"test_dataset_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the processed test set and print WER and loss\n",
    "from datasets import load_from_disk\n",
    "\n",
    "test_dataset = load_from_disk(\"test_dataset_processed\")\n",
    "results = trainer.evaluate(test_dataset=test_dataset)\n",
    "print(f\"Test Loss: {results['eval_loss']:.4f}\")\n",
    "if 'eval_wer' in results:\n",
    "    print(f\"Test WER: {results['eval_wer']:.4f}\")\n",
    "else:\n",
    "    print(\"WER not computed. Check compute_metrics function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a01c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and processor for later use\n",
    "model.save_pretrained(\"./whisper-vi-finetuned\")\n",
    "processor.save_pretrained(\"./whisper-vi-finetuned\")\n",
    "print(\"Model and processor saved to ./whisper-vi-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ed982",
   "metadata": {},
   "source": [
    "---\n",
    "This notebook provides a basic pipeline for fine-tuning Whisper on Vietnamese ASR data. You can further customize preprocessing, augmentation, and hyperparameters as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed2ffe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
